{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU (Cuda) power !\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import joblib\n",
    "import itertools\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset, SubsetRandomSampler\n",
    "from torch import Generator\n",
    "from torch.utils.data.dataset import random_split\n",
    "# pip install transformers et pip instance sentencepiece\n",
    "import transformers as ppb\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "# visualisation :\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import time\n",
    "# General settings\n",
    "allow_cuda = True\n",
    "warnings.filterwarnings('ignore')\n",
    "# Check whether cuda is available, and select processor or GPU\n",
    "if torch.cuda.is_available() and allow_cuda:\n",
    "    print('GPU (Cuda) power !')\n",
    "    hardware = torch.device('cuda:0')\n",
    "    print(hardware)\n",
    "    torch.cuda.set_device(0)\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print('CPU running')\n",
    "    hardware = torch.device('cpu')\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "labels_list = ['+', '-', '0', 'i', 'j', 'f', 's', 'p', 'm', 'a', 't']\n",
    "labels_nb = len(labels_list)\n",
    "\n",
    "# Fonctions communes\n",
    "def print_cuda_memory_usage():\n",
    "    if hardware == torch.device('cuda:0'):\n",
    "        t = torch.cuda.get_device_properties(0).total_memory / 1073741824\n",
    "        r = torch.cuda.memory_reserved(0) / 1048576\n",
    "        a = torch.cuda.memory_allocated(0) / 1048576\n",
    "        print('cuda total=', t, 'reserved=', r, 'allocated=', a, 'free=', r - a, '...quelle unité ?')\n",
    "\n",
    "# Log a string, and store to globals the log filename\n",
    "def log(line, log_file='logs/default.txt'):\n",
    "    print(line)\n",
    "    if not ('logfile' in globals()):\n",
    "        global logfile\n",
    "        logfile = log_file\n",
    "        print('set logfile to ', logfile)\n",
    "    with open(logfile, \"a\") as f:\n",
    "        print(line, file=f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stockage des donnees dans un TensorDataset, permet aussi de produire les minibatches\n",
    "# dataframe pandas en entrée au lieu d'un chemin vers un fichier\n",
    "class Textes_TensorDataset(TensorDataset):\n",
    "    def __init__(self, textes_df, tokenizer):\n",
    "        super(TensorDataset, self).__init__()\n",
    "        self.df = textes_df\n",
    "        self.tokenizer = tokenizer\n",
    "        # get the CLS token to check it\n",
    "        cls_sep_tokens = tokenizer.encode('', add_special_tokens=True)\n",
    "        tokenized = self.df['phrases'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "        max_len = 0\n",
    "        for i in tokenized.values:\n",
    "            if len(i) > max_len:\n",
    "                max_len = len(i)\n",
    "        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized.values])\n",
    "        \n",
    "\n",
    "        self.input_ids = padded\n",
    "        print('input_ids.shape', self.input_ids.shape)\n",
    "        self.attention_mask = np.where(padded != 0, 1, 0)\n",
    "        self.max_len = self.input_ids.shape[1]\n",
    "        self.nb_textes = self.input_ids.shape[0]\n",
    "\n",
    "        # Préparation des labels à partir des colonnes (troisième colonne)\n",
    "        self.raw_labels_names = self.df.columns[2:]\n",
    "        for i, row in self.df.iterrows():\n",
    "            for key in labels_list:\n",
    "                self.df.at[i, key] = max([row[c] for c in self.raw_labels_names if key in c])\n",
    "        print('Textes_TensorDataset/__init()__ loaded', self.nb_textes, 'texts', self.max_len, 'of chars length')\n",
    "\n",
    "    # renvoie un item : input_id, attention_mask, labels, last_hidden_states (si disponibles)\n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.attention_mask[index], self.df.loc[index, labels_list].to_numpy(dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nb_textes\n",
    "\n",
    "#\n",
    "# Class CamemBERT avec couche linear on-top\n",
    "#\n",
    "class CustomBertModel(torch.nn.Module):\n",
    "    def __init__(self, bert_model, weights, layer_sizes):\n",
    "        super(CustomBertModel, self).__init__()\n",
    "        self.layer_sizes = layer_sizes\n",
    "\n",
    "        self.bert = bert_model.from_pretrained(weights)\n",
    "        self.dropout = torch.nn.Dropout(.05)\n",
    "        self.linear1 = torch.nn.Linear(1 * 768, self.layer_sizes[0])\n",
    "        self.nonlinear1 = torch.nn.Tanh()\n",
    "        self.linear2 = torch.nn.Linear(self.layer_sizes[0], self.layer_sizes[1])\n",
    "        self.nonlinear2 = torch.nn.Tanh() # attention Tanh donne de meilleurs résultats que Sigmoid\n",
    "        self.linear3 = torch.nn.Linear(self.layer_sizes[1], self.layer_sizes[2])\n",
    "        print('CustomBertModel/__init__ with layers on top', layer_sizes)\n",
    "\n",
    "    # forwarde le modele, mode = 0 ne sort que la sortie par defaut, mode = 1 sort la sortie de bert aussi\n",
    "    def forward(self, input_ids, attention_mask, mode = 0):\n",
    "        bert_output = self.bert(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        # Ne recupere le last_hidden_state que du CLS token\n",
    "        lhd = bert_output.last_hidden_state\n",
    "        lhd_cls_token = lhd[:, 0, :]\n",
    "        \n",
    "        # Linear layer on top of BERT\n",
    "        x = self.dropout(lhd_cls_token)\n",
    "        x = self.nonlinear1(self.linear1(x))\n",
    "        x = self.nonlinear2(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        \n",
    "        if mode == 0:\n",
    "            return x\n",
    "        elif mode == 1:\n",
    "            return x, lhd_cls_token\n",
    "\n",
    "    # Bloque ou permet l'apprentissage sur la partie bert du modele\n",
    "    def bert_training(self, authorisation, tune_depth):\n",
    "        for name, p in self.named_parameters():\n",
    "            p.requires_grad = False\n",
    "            if (\"linear1\" in name) or (\"linear2\" in name) or (\"linear3\" in name): \n",
    "                p.requires_grad = authorisation\n",
    "            if(tune_depth < 12) and (tune_depth >= 0) :\n",
    "                for depth in np.arange(tune_depth, 12):\n",
    "                    if (\"bert.encoder.layer.\"+ str(depth) in name):\n",
    "                        p.requires_grad = authorisation\n",
    "                if (\"bert.pooler.dense\" in name) :\n",
    "                    p.requires_grad = authorisation\n",
    "\n",
    "    # Test on a batch, return loss\n",
    "    def evaluate(self, x, attention_mask, expected, criterion):\n",
    "        #print('EVALUATE :')\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            output = self.forward(x, attention_mask=attention_mask)\n",
    "            loss_train = criterion(output, expected)\n",
    "            xx = np.rint(100 * expected[:, :].cpu().numpy())\n",
    "            yy = np.rint(100 * torch.sigmoid(output[:, :]).cpu().numpy())\n",
    "        return loss_train.item(), xx, yy\n",
    "    # Train on a batch, return loss\n",
    "    def learn(self, x, attention_mask, expected, criterion, optimizer):\n",
    "        self.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = self.forward(x, attention_mask=attention_mask)\n",
    "        loss_train = criterion(output, expected)\n",
    "        loss_train.backward()  # Compute the back propagation gradients\n",
    "        optimizer.step()  # Setup coefficients\n",
    "        return loss_train.item()\n",
    "\n",
    "    def save(self, fn):\n",
    "        torch.save(self.state_dict(), fn)\n",
    "\n",
    "    def load(self, fn):\n",
    "        self.load_state_dict(torch.load(fn))\n",
    "        self.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camembert <class 'transformers.models.camembert.modeling_camembert.CamembertModel'>\n",
      "Tokenizer <class 'transformers.models.camembert.tokenization_camembert.CamembertTokenizer'>\n",
      "input_ids.shape (1648, 345)\n",
      "Textes_TensorDataset/__init()__ loaded 1648 texts 345 of chars length\n",
      "1648\n"
     ]
    }
   ],
   "source": [
    "date_start = datetime.now().strftime(\"%Y%m%d_%Hh%M\")\n",
    "model_name = f'{date_start}'\n",
    "\n",
    "# load model, tokenizer and weights\n",
    "camembert, tokenizer, weights = (ppb.CamembertModel, ppb.CamembertTokenizer, 'camembert-base')\n",
    "print('Camembert', camembert)\n",
    "print('Tokenizer', tokenizer)\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer.from_pretrained(weights)\n",
    "\n",
    "# mettre de côté un jeu de données test avec une graine du générateur \n",
    "# charger le jeu de données en TensorDataset\n",
    "data_df = pd.read_csv('DonneesPedoPsy/labeled_data.csv', sep=\";\", encoding='cp1252')\n",
    "data_df.drop('num', axis=1, inplace=True)\n",
    "textes_td = Textes_TensorDataset(data_df, tokenizer)\n",
    "print(textes_td.nb_textes)\n",
    "text_td_train_nb = int(textes_td.nb_textes * .8)\n",
    "text_td_test_nb = textes_td.nb_textes - text_td_train_nb\n",
    "#train_df, test_df = train_test_split(data_df, train_size=text_td_train_nb,\n",
    "#                                     test_size=text_td_test_nb, random_state=11)\n",
    "# Construit des datasets d'entrainement et de test\n",
    "#train_df.reset_index(inplace=True)\n",
    "# ATTENTION modifier pour rendre disjoints les jeux de validation !!!!\n",
    "textes_td_train, textes_td_test = random_split(textes_td, [text_td_train_nb, text_td_test_nb],\n",
    "                                               generator=torch.Generator(device=hardware).manual_seed(11))\n",
    "print(len(textes_td_train.indices))\n",
    "print(len(textes_td_test.indices))\n",
    "joblib.dump(textes_td_train.indices, 'train_indices.sav')    \n",
    "joblib.dump(textes_td_test.indices, 'test_indices.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(current_train, current_val, hyperp):\n",
    "    current_train_nb = len(current_train)\n",
    "    print(current_train_nb)\n",
    "    current_val_nb = len(current_val)\n",
    "    print(current_val_nb)\n",
    "    batch_size_train = int(hyperp['bs_train'])\n",
    "    batch_size_val = int(hyperp['bs_val'])\n",
    "    tune_depth = int(hyperp['tune_depth'])\n",
    "\n",
    "    # model = camembert.from_pretrained(weights)\n",
    "    model = CustomBertModel(camembert, weights=weights, layer_sizes=[int(hyperp['layer_1']), int(hyperp['layer_2']), labels_nb])\n",
    "\n",
    "    # Define optimized parameters\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction='sum', pos_weight=torch.ones([len(labels_list)]))\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=hyperp['lr'].item())\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=1, gamma=1) # gamma décroissance de \"lr\"\n",
    "    model.bert_training(True, tune_depth)\n",
    "    # containers pour l'entrainement\n",
    "    record_train_step = list()\n",
    "    record_train_loss = list()\n",
    "    record_val_loss = list()\n",
    "    record_learning_rate = list()\n",
    "\n",
    "    #time.sleep(1)\n",
    "    # Parametres du early stopping mechanism\n",
    "    epoch_min = 0\n",
    "    early_stopping_patience = int(hyperp['esp'])  # Nombre de fois avant que le early stopping s'actionne\n",
    "    early_stopping_count = 0\n",
    "    loss_val_min, loss_train_min = 1000000, 1000000\n",
    "\n",
    "    for epoch in range(int(hyperp['nb_epoch'])):\n",
    "        record_train_step.append(epoch)\n",
    "        # Entraine le modele\n",
    "        loss_train = 0\n",
    "        sample_done = 0\n",
    "        for input_ids, attention_masks, labels in DataLoader(dataset=current_train, batch_size=batch_size_train,\n",
    "                                                             shuffle=True, generator=Generator(device=hardware)):\n",
    "            loss_train += model.learn(input_ids, attention_masks, labels, loss_fn, optim)\n",
    "            sample_done += batch_size_train\n",
    "       \n",
    "    \n",
    "        loss_train /= current_train_nb\n",
    "        record_train_loss.append(loss_train)\n",
    "        record_learning_rate.append(scheduler.get_last_lr())\n",
    "        scheduler.step()\n",
    "        #Valide sur les donnees de validation x, attention_mask, expected, criterion, optimizer\n",
    "        loss_val = 0\n",
    "        sample_done = 0\n",
    "        sklearn_ytrue, sklearn_ypred = [], []\n",
    "        for input_ids, attention_masks, labels in DataLoader(dataset=current_val, batch_size=batch_size_val,\n",
    "                                                             shuffle=False):\n",
    "            loss_val_batch, xx, yy = model.evaluate(input_ids, attention_masks, labels, loss_fn)\n",
    "            loss_val += loss_val_batch\n",
    "            sample_done += batch_size_val\n",
    "            # To compute Precision/recall msklearn metrics\n",
    "            sklearn_ytrue.append(labels.cpu().numpy())\n",
    "            sklearn_ypred.append(yy > 50)\n",
    "            \n",
    "        loss_val /= current_val_nb\n",
    "        record_val_loss.append(loss_val)\n",
    "        #\n",
    "        sklearn_ytrue = np.vstack(sklearn_ytrue)\n",
    "        sklearn_ypred = np.vstack(sklearn_ypred)\n",
    "        if loss_train_min > loss_train:\n",
    "            loss_train_min = loss_train\n",
    "        # Early stopping\n",
    "        if loss_val_min < loss_val:\n",
    "            early_stopping_count += 1\n",
    "            if early_stopping_count >= early_stopping_patience:\n",
    "                break\n",
    "        else:\n",
    "            early_stopping_count = 0\n",
    "            epoch_min = epoch\n",
    "            loss_val_min = loss_val\n",
    "    model.save('Models/adopsy_fine_tuning_train_data.m3')  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1318\n",
      "330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomBertModel/__init__ with layers on top [200, 110, 11]\n"
     ]
    }
   ],
   "source": [
    "unpickled_df = pd.read_pickle(\"repeated_5folds_cv.pkl\")\n",
    "m = np.zeros(len(unpickled_df[0][\"cv_error\"]))\n",
    "for i in range(len(unpickled_df)) : \n",
    "     m=m + unpickled_df[i][\"cv_error\"]\n",
    "mymin =  np.argmin(m/len(unpickled_df)) \n",
    "hyper_param_optim = unpickled_df[0].iloc[[mymin]]\n",
    "#print(hyper_param_optim['lr'].item())\n",
    "model = fit_model(current_train=textes_td_train, current_val=textes_td_test, hyperp=hyper_param_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(datatest, model, batch_size=64):\n",
    "    batch_size = len(datatest)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction='sum', pos_weight=torch.ones([len(labels_list)]))\n",
    "    ytrue, ypred = [], []\n",
    "    for input_ids, attention_masks, labels in DataLoader(dataset=datatest, batch_size=batch_size,\n",
    "                                                             shuffle=False):\n",
    "        loss_val_batch, xx, yy = model.evaluate(input_ids, attention_masks, labels, loss_fn)\n",
    "        #loss_val += loss_val_batch\n",
    "        #sample_done += batch_size_val\n",
    "        # To compute Precision/recall msklearn metrics\n",
    "        ytrue.append(labels.cpu().numpy())\n",
    "        ypred.append(yy > 50)\n",
    "    return np.array(ytrue)[0], np.array(ypred).astype(float)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification par fine tuning \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           +       0.67      0.82      0.74        72\n",
      "           -       0.85      0.37      0.51        63\n",
      "           0       0.40      0.38      0.39        47\n",
      "           i       0.78      0.81      0.79       146\n",
      "           j       0.86      0.97      0.91       263\n",
      "           f       0.91      0.89      0.90        36\n",
      "           s       0.97      0.98      0.97        57\n",
      "           p       0.97      0.93      0.95       124\n",
      "           m       0.94      0.93      0.93       123\n",
      "           a       0.91      0.86      0.89        72\n",
      "           t       0.78      0.61      0.68        23\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      1026\n",
      "   macro avg       0.82      0.78      0.79      1026\n",
      "weighted avg       0.85      0.84      0.84      1026\n",
      " samples avg       0.83      0.81      0.81      1026\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8376288298503156"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "ytrue, ypred = predict(textes_td_test, model, 128)\n",
    "\n",
    "print('classification par fine tuning \\n',classification_report(ytrue, ypred,target_names=labels_list))\n",
    "f1_score(ytrue, ypred, average=\"weighted\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "040f19a775087cd85403a0e0795228cc507b245762d9d1c381cf34247a53faf9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
